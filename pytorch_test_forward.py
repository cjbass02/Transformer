# main.py
import torch
import torch.nn.functional as F
from pytorch_transformer.transformer import Transformer
from dummy_vocab import dummy_vocab  # Import our large dummy vocabulary


# generated by chat GPT to get word completion working

# Set vocab_size based on our dummy vocabulary.
vocab_size = len(dummy_vocab)

def generate_text(model, prompt_tokens, max_gen_length=10, top_k=5):
    """
    Generate text tokens using the autoregressive decoder-only Transformer.
    
    Args:
        model: the Transformer model.
        prompt_tokens: list of token indices to start the prompt.
        max_gen_length: number of tokens to generate.
        top_k: number of top tokens to consider (greedy if top_k=1).
    Returns:
        List of token indices (prompt + generated tokens).
    """
    model.eval()
    generated = prompt_tokens.copy()  # start with the prompt
    for _ in range(max_gen_length):
        # Prepare input: shape (1, current_seq_length)
        x = torch.tensor([generated], dtype=torch.long)
        logits, _ = model(x)
        # Get logits for the last token.
        last_logits = logits[:, -1, :]  # shape: (1, vocab_size)
        probs = F.softmax(last_logits, dim=-1)
        # Greedy selection: choose the token with the highest probability.
        top_probs, top_indices = torch.topk(probs, top_k, dim=-1)
        next_token = top_indices[0, 0].item()
        generated.append(next_token)
    return generated

def tokens_to_text(tokens, vocab):
    """
    Convert a list of token indices to a text string using the vocabulary.
    """
    words = [vocab.get(token, "<UNK>") for token in tokens]
    return " ".join(words)

def main():
    # Model hyperparameters. (Using a smaller model for demonstration.)
    d_model = 32       # Embedding dimension.
    num_heads = 4      # Number of attention heads.
    d_ff = 64          # Feed-forward network dimension.
    num_layers = 2     # Number of decoder blocks.
    max_len = 50       # Maximum sequence length.

    # Instantiate the decoder-only Transformer.
    model = Transformer(vocab_size, d_model, num_heads, d_ff, num_layers, max_len)

    # Define a prompt using token indices that form an actual sentence.
    # "once upon time in a land far away there lived a brave knight"
    prompt_tokens = [100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112]

    # Generate text completion.
    generated_tokens = generate_text(model, prompt_tokens, max_gen_length=10, top_k=5)
    generated_text = tokens_to_text(generated_tokens, dummy_vocab)

    print("Prompt:", tokens_to_text(prompt_tokens, dummy_vocab))
    print("Generated Text:", generated_text)

if __name__ == '__main__':
    main()
