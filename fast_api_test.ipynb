{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94814fbc",
   "metadata": {},
   "source": [
    "# Fast API Test\n",
    "\n",
    "This is a test for my transformer API. It is a simple test to check if the API is working correctly. The training process will be **very** slow, but just a proof of concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4ffb097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "SEQUENCE_LENGTH = 200\n",
    "API_URL = \"http://localhost:8080\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c85abcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "def tokenize(text):\n",
    "    return list(text)  \n",
    "\n",
    "def build_vocab(tokens, max_size=None):\n",
    "    freq = Counter(tokens)\n",
    "    most_common = freq.most_common(max_size)\n",
    "    vocab = {tok: i+1 for i, (tok, _) in enumerate(most_common)}\n",
    "    vocab['<unk>'] = 0\n",
    "    inv_vocab = {i: tok for tok, i in vocab.items()}\n",
    "    return vocab, inv_vocab\n",
    "\n",
    "def encode(tokens, vocab):\n",
    "    return [vocab.get(tok, vocab['<unk>']) for tok in tokens]\n",
    "\n",
    "\n",
    "def make_sequences(ids, seq_len=SEQUENCE_LENGTH):\n",
    "    inputs, targets = [], []\n",
    "    for i in range(len(ids) - seq_len):\n",
    "        inputs.append(ids[i:i+seq_len])\n",
    "        targets.append(ids[i+1:i+1+seq_len])\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "272b55db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, d_model=128, num_heads=4, d_ff=512, num_layers=1, max_len=5000):\n",
    "    payload = {\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"d_model\": d_model,\n",
    "        \"num_heads\": num_heads,\n",
    "        \"d_ff\": d_ff,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"max_len\": max_len\n",
    "    }\n",
    "    resp = requests.post(f\"{API_URL}/create\", json=payload)\n",
    "    resp.raise_for_status()\n",
    "    print(resp.json())\n",
    "\n",
    "def train_model(inputs, targets, epochs=1, lr=1e-3):\n",
    "    batch_size = 64\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for i in range(0, len(inputs), batch_size):\n",
    "            batch_in = inputs[i:i+batch_size]\n",
    "            batch_tg = targets[i:i+batch_size]\n",
    "            payload = {\n",
    "                \"token_sequences\": batch_in,\n",
    "                \"target_sequences\": batch_tg,\n",
    "                \"epochs\": 1,\n",
    "                \"learning_rate\": lr\n",
    "            }\n",
    "            resp = requests.post(f\"{API_URL}/train\", json=payload)\n",
    "            resp.raise_for_status()\n",
    "        print(resp.json())\n",
    "\n",
    "\n",
    "def autocomplete(prefix, vocab, inv_vocab, max_tokens=50):\n",
    "    # encode prefix\n",
    "    tokens = tokenize(prefix)\n",
    "    ids = encode(tokens, vocab)\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        resp = requests.post(f\"{API_URL}/predict\", json={\"input_tokens\": ids[-SEQUENCE_LENGTH:]})\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        next_id = data['predicted_token']\n",
    "        next_tok = inv_vocab.get(next_id, '<unk>')\n",
    "        tokens.append(next_tok)\n",
    "        ids.append(next_id)\n",
    "        if next_tok in ['.', '!', '?']:\n",
    "            break\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6a8ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load and preprocess\n",
    "text = load_corpus('input_short.txt')\n",
    "toks = tokenize(text)\n",
    "vocab, inv_vocab = build_vocab(toks)\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "# 2) Build sequences\n",
    "ids = encode(toks, vocab)\n",
    "inputs, targets = make_sequences(ids)\n",
    "print(f\"Total sequences: {len(inputs)}\")\n",
    "\n",
    "# 3) Initialize model\n",
    "create_model(vocab_size=len(vocab), d_model=256, num_heads=8, d_ff=1024, num_layers=1)\n",
    "\n",
    "# 4) Train\n",
    "train_model(inputs, targets, epochs=1, lr=0.01)\n",
    "\n",
    "# 5) Test autocomplete\n",
    "prompt = \"once upon a time\"\n",
    "completion = autocomplete(prompt, vocab, inv_vocab)\n",
    "print(\"\\nCompletion:\", completion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
